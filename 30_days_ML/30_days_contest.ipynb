{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d142247a-42ab-4d60-a808-5f996f57ad67",
   "metadata": {
    "tags": []
   },
   "source": [
    "Welcome to the **[30 Days of ML competition](https://www.kaggle.com/c/30-days-of-ml/overview)**!  In this notebook, you'll learn how to make your first submission.\n",
    "\n",
    "Before getting started, make your own editable copy of this notebook by clicking on the **Copy and Edit** button.\n",
    "\n",
    "# Step 1: Import helpful libraries\n",
    "\n",
    "We begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https://www.kaggle.com/learn/intermediate-machine-learning)** course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9353dc2e-f071-430b-a714-9eddcfae6eb1",
   "metadata": {},
   "source": [
    "### Tensorflow\n",
    "Including GPU support - sometimes - having trouble keeping tf-gpu working in Anaconda on Windoze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aed10f6-6d10-4e86-8111-9ff270d695d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(\n",
    "    gpu_options=tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=0.8),\n",
    "    device_count={'GPU': 1},\n",
    "    # session = tf.compat.v1.Session(config=config) \n",
    "    # tf.compat.v1.keras.backend.set_session(session)\n",
    ")\n",
    "\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(session)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaa035e-98de-4558-8885-7b6146b948d8",
   "metadata": {
    "id": "JWibA4lcO-P-"
   },
   "source": [
    "### Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590d4a1-ce12-4a73-9751-72acedfcc795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data management\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import statsmodels.api as sm\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Regressors\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# suppress \"torch\" warning in TPOT\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42d614e-a197-40e9-b229-6f30a1c56ca0",
   "metadata": {},
   "source": [
    "# Step 2: Load the data\n",
    "\n",
    "Next, we'll load the training and test data.  \n",
    "\n",
    "We set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a24c66-ef8d-4478-b4e6-9228a3dca6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "path = \"\"  # \"../input/30-days-of-ml/\"\n",
    "df_train = pd.read_csv(f\"{path}train.csv\")\n",
    "df_test = pd.read_csv(f\"{path}test.csv\")\n",
    "\n",
    "# Preview the data\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746dcd7d-5760-4c69-bf3c-461dc615ae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6645b1-43ee-4ddc-9af7-f55e3c2588ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"kfold\"] = -1\n",
    "\n",
    "kf = model_selection.KFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "for fold, (train_indicies, valid_indicies) in enumerate(kf.split(X=df_train)):\n",
    "    df_train.loc[valid_indicies, \"kfold\"] = fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60862529-38c0-47a8-9267-4a05df742447",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f18088a-bb2b-4060-89ed-87efec8afe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target from features\n",
    "y = df_train['target']\n",
    "features = df_train.drop(['target'], axis=1)\n",
    "\n",
    "# Preview features\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b473c0-94bb-4565-8aad-513359cbc4c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 3: Prepare the data\n",
    "\n",
    "Next, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n",
    "\n",
    "In the **[Categorical Variables lesson](https://www.kaggle.com/alexisbcook/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca734432-0b22-47c4-8607-d8b6ebe3ffc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categorical columns\n",
    "object_cols = [col for col in features.columns if 'cat' in col]\n",
    "\n",
    "# ordinal-encode categorical columns\n",
    "X = features.copy()\n",
    "X_test = df_test.copy()\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "X[object_cols] = ordinal_encoder.fit_transform(features[object_cols])\n",
    "X_test[object_cols] = ordinal_encoder.transform(df_test[object_cols])\n",
    "\n",
    "# Preview the ordinal-encoded features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c61e4f-574e-46ab-961c-665b4d84a661",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95690d1-d568-448a-be28-d623f99da5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "y_train_enc = lab_enc.fit_transform(y_train)\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "y_valid_enc = lab_enc.fit_transform(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297fba09-6802-4196-a153-cf770dceb849",
   "metadata": {},
   "source": [
    "# Step 4: Train a model\n",
    "\n",
    "Now that the data is prepared, the next step is to train a model.  \n",
    "\n",
    "If you took the **[Intro to Machine Learning](https://www.kaggle.com/learn/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https://www.kaggle.com/dansbecker/random-forests)**.  In the code cell below, we fit a random forest model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceedf25-a982-456a-99ab-60f2490e8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_rf = False\n",
    "\n",
    "if run_rf:\n",
    "    model = RandomForestRegressor(random_state=1)\n",
    "\n",
    "    # Train the model (will take about 10 minutes to run)\n",
    "    %time model.fit(X_train, y_train)\n",
    "\n",
    "    pred_rf = model.predict(X_valid)\n",
    "    print(mean_squared_error(y_valid, pred_rf, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5b878f-0698-464d-b120-8f82b3d8d967",
   "metadata": {},
   "source": [
    "In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5abbd9c-8e97-48fd-a204-55f52d5b0cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215f2514-047b-4a0a-81cc-b8f119ddcc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_xgb_search = True\n",
    "\n",
    "if run_xgb_search:\n",
    "    # Feed the XGB into the model pipeline\n",
    "    my_pipeline = Pipeline(\n",
    "        [\n",
    "         # ('imputer', Imputer()),\n",
    "         ('xgbrg', XGBRegressor())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_grid = {\n",
    "        \"xgbrg__n_estimators\": [5000, 10000],\n",
    "        \"xgbrg__learning_rate\": [0.05, 0.1],\n",
    "        \"xgbrg__subsample\": [0.8],\n",
    "        \"xgbrg__colsample_bytree\": [0.2],\n",
    "        \"xgbrg__max_depth\": [3, 5],\n",
    "        \"xgbrg__booster\": ['gbtree'],\n",
    "        \"xgbrg__reg_lambda\": [0.2, 0.4, 0.6],\n",
    "        \"xgbrg__reg_alpha\": [13, 15],\n",
    "        \"xgbrg__random_state\": [42],\n",
    "        \"xgbrg__n_jobs\": [-1],\n",
    "        \"xgbrg__gpu_id\": [0],\n",
    "        \"xgbrg__tree_method\": ['gpu_hist'],\n",
    "        # \"xgbrg__verbosity\": [1]\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    params = {\n",
    "        'learning_rate': 0.07853392035787837,\n",
    "        'reg_lambda': 1.7549293092194938e-05,\n",
    "        'reg_alpha': 14.68267919457715,\n",
    "        'subsample': 0.8031450486786944,\n",
    "        'colsample_bytree': 0.170759104940733,\n",
    "        'max_depth': 3\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    searchCV = GridSearchCV(\n",
    "        my_pipeline,\n",
    "        cv=3,\n",
    "        param_grid=param_grid,\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    searchCV.fit(\n",
    "        X_train, y_train,\n",
    "        xgbrg__early_stopping_rounds=300,\n",
    "        xgbrg__eval_set=[(X_valid, y_valid)],\n",
    "        xgbrg__verbose=1000\n",
    "    )\n",
    "\n",
    "    print((time.time() - start)/60.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543eede1-9808-47a7-8c1d-be0db5f22293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the parameters which yield the best model performance\n",
    "print(searchCV.best_estimator_)\n",
    "print(searchCV.best_score_)\n",
    "print(searchCV.best_params_)\n",
    "# print(pd.DataFrame(grid.cv_results_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152acd94-17b7-41cc-b3dc-6c73a20fd320",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_xgb = False\n",
    "\n",
    "if run_xgb:\n",
    "    xgb_parameters = {\n",
    "        'n_estimators': 5000,\n",
    "        'learning_rate': 0.05,\n",
    "        'n_jobs': -1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.2,\n",
    "        'max_depth': 3,\n",
    "        'booster': 'gbtree',\n",
    "        'reg_lambda': 0.2,\n",
    "        'reg_alpha': 15,\n",
    "        'random_state': 42,\n",
    "        # 'gpu_id': 0,\n",
    "        # 'tree_method': 'gpu_hist',\n",
    "        # 'predictor': 'gpu_predictor'\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    params = {\n",
    "        'learning_rate': 0.07853392035787837,\n",
    "        'reg_lambda': 1.7549293092194938e-05,\n",
    "        'reg_alpha': 14.68267919457715,\n",
    "        'subsample': 0.8031450486786944,\n",
    "        'colsample_bytree': 0.170759104940733,\n",
    "        'max_depth': 3\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    xgb_model = XGBRegressor(**xgb_parameters)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    xgb_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        early_stopping_rounds=300,\n",
    "        verbose=1000,\n",
    "    )\n",
    "\n",
    "    print((time.time()-start)/60.0)\n",
    "\n",
    "    pred_xgb = xgb_model.predict(X_valid)\n",
    "\n",
    "    print(mean_squared_error(y_valid, pred_xgb, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc98eb3-a1d5-47fc-9251-56d7006c4c78",
   "metadata": {},
   "source": [
    "### Using Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5386c3-a43e-4065-9a17-8a5ab4cc0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lgbm = False\n",
    "\n",
    "if run_lgbm:\n",
    "    from lightgbm import LGBMRegressor\n",
    "\n",
    "    lgbm_parameters = {\n",
    "        'metric': 'rmse',\n",
    "        'n_jobs': -1,\n",
    "        'n_estimators': 10000,\n",
    "        'reg_alpha': 10.924491968127692,\n",
    "        'reg_lambda': 17.396730654687218,\n",
    "        'colsample_bytree': 0.21497646795452627,\n",
    "        'subsample': 0.7582562557431147,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': 12,\n",
    "        'num_leaves': 32,\n",
    "        'min_child_samples': 16,\n",
    "        'max_bin': 256,\n",
    "        'cat_l2': 0.025083670064082797\n",
    "    }\n",
    "\n",
    "    lgbm_model = LGBMRegressor(**lgbm_parameters)\n",
    "    lgbm_model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=((X_valid, y_valid)),\n",
    "        verbose=-1,\n",
    "        early_stopping_rounds=64,\n",
    "        categorical_feature=object_cols\n",
    "    )\n",
    "\n",
    "    pred_lgbm = lgbm_model.predict(X_valid)\n",
    "\n",
    "    print(mean_squared_error(y_valid, pred_lgbm, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51519763-5cd3-4c05-8c55-8afb45aa7a03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TPOT to find best solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e296581-66d7-472b-a3f1-4edab52015ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ae0dac-1693-4bc0-bafa-140f858d4070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPOT for classification\n",
    "from tpot import TPOTClassifier\n",
    "\n",
    "# Instantiate and train a TPOT auto-ML classifier\n",
    "tpot = TPOTClassifier(\n",
    "    generations=1,\n",
    "    population_size=5,\n",
    "    subsample=0.05,\n",
    "    # config_dict='TPOT cuML',\n",
    "    verbosity=2,\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "%time tpot.fit(X_train, y_train_enc)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Export the optimized pipeline as Python code.\n",
    "tpot.export('tpot_products_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83b982e-cf83-4be6-8daf-b9ad5a144062",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step 5: Submit to the competition\n",
    "\n",
    "We'll begin by using the trained model to generate predictions, which we'll save to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49cd437-e84d-4de1-b71c-e853c970802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to generate predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output = pd.DataFrame({'Id': X_test.index,\n",
    "                       'target': predictions})\n",
    "\n",
    "output.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bee64d-7b41-472d-9196-7ff4fc3240c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'learning_rate': 0.07853392035787837,\n",
    "    'reg_lambda': 1.7549293092194938e-05,\n",
    "    'reg_alpha': 14.68267919457715,\n",
    "    'subsample': 0.8031450486786944,\n",
    "    'colsample_bytree': 0.170759104940733,\n",
    "    'max_depth': 3\n",
    "}\n",
    "\n",
    "model = XGBRegressor(\n",
    "    random_state=0, \n",
    "    #tree_method='gpu_hist',\n",
    "    #gpu_id=0,\n",
    "    #predictor=\"gpu_predictor\",\n",
    "    n_estimators=5000,\n",
    "    **params\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    xtrain, ytrain,\n",
    "    early_stopping_rounds=300,\n",
    "    eval_set=[(xvalid, yvalid)],\n",
    "    verbose=1000\n",
    ")\n",
    "\n",
    "preds_valid = model.predict(xvalid)\n",
    "test_preds = model.predict(xtest)\n",
    "final_predictions.append(test_preds)\n",
    "rmse = mean_squared_error(yvalid, preds_valid, squared=False)\n",
    "print(fold, rmse)\n",
    "scores.append(rmse)\n",
    "\n",
    "print(np.mean(scores), np.std(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
